name: Predict on new data or with new model

on: 
  push:
    branches:
      - main
    paths:  # predict when the test set has changed, as well as when the model artifact is updated
      - 'data/raw/test.csv'
      - 'data/processed/test.csv'
      - 'ARISA_DSML/predict.py'
      - 'models/catboost_model_titanic.cbm'
  workflow_dispatch:  # Allows manual triggering from the train workflow
jobs:
  preprocess:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout repository
        uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with: 
          python-version: "3.11"
      - name: Install dependencies
        run: make requirements
      - name: Create kaggle directory
        env: 
          KAGGLE_KEY: ${{ secrets.KAGGLE_KEY }}
        run: |
          mkdir -p $HOME/.config
          mkdir -p $HOME/.config/kaggle
          echo "$KAGGLE_KEY" > $HOME/.config/kaggle/kaggle.json
          chmod 600 $HOME/.config/kaggle/kaggle.json
      - name: Run preprocessing
        run: make preprocess

      - name: Upload preprocessed data
        uses: actions/upload-artifact@v4
        with:
          name: processed-data
          path: data/processed
  predict:
    runs-on: ubuntu-latest
    needs: preprocess
    permissions:
      contents: write  # This gives the token write access to the repository contents
    steps: 
      - name: Checkout repository
        uses: actions/checkout@v4

      - name: Download processed data
        uses: actions/download-artifact@v4
        with:
          name: processed-data
          path: data/processed
      - name: Set up Python
        uses: actions/setup-python@v4
        with: 
          python-version: "3.11"
      - name: Install dependencies
        run: make requirements
      - name: Resolve challenge
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: make resolve
      - name: Predict on test data
        env:
          MLFLOW_TRACKING_URI: ${{ secrets.MLFLOW_TRACKING_URI }}
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: make predict
      - name: Upload predictions
        uses: actions/upload-artifact@v4
        with:
          name: predictions
          path: models/preds.csv
      - name: Install AWS CLI
        run: |
          pip install --upgrade awscli

      - name: Upload predictions to S3
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          ARTIFACT_BUCKET: ${{ secrets.ARTIFACT_BUCKET }}
        run: |
          aws s3 cp models/preds.csv s3://$ARTIFACT_BUCKET/predictions/preds.csv --acl private
